# -*- coding: utf-8 -*-
"""Final DPO_Project_Mistral.ipynb

Automatically generated by Colaboratory.

"""

import torch
torch.cuda.is_available()

!pip install -q datasets trl peft bitsandbytes sentencepiece wandb

!pip install -qU accelerate

import os
import gc
import torch

import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from trl import DPOTrainer
import bitsandbytes as bnb
from google.colab import userdata
import wandb
from typing import Dict, Optional
from datasets import Dataset, load_dataset
from tqdm import tqdm


# use colab secret tab
hf_token = userdata.get('huggingface')
wb_token = userdata.get('wandb')
wandb.login(key=wb_token)

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
new_model = "mistral-7B-instruct-dpo"
my_dpo_model = "mitultiwari/mistral-7B-instruct-dpo"

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

# Model to fine-tune
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)

# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']
)

model.config.use_cache = False

# Reference model
ref_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    load_in_4bit=True
)

# Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    learning_rate=5e-5,
    lr_scheduler_type="cosine",
    max_steps=300,
    save_strategy="no",
    logging_steps=1,
    output_dir=new_model,
    optim="paged_adamw_32bit",
    warmup_steps=100,
    bf16=True,
    report_to="wandb",
)

#prepare dataset
def extract_anthropic_prompt(prompt_and_response):
    """Extract the anthropic prompt from a prompt and response pair."""
    search_term = "\n\nAssistant:"
    search_term_idx = prompt_and_response.rfind(search_term)
    assert search_term_idx != -1, f"Prompt and response does not contain '{search_term}'"
    return prompt_and_response[: search_term_idx + len(search_term)]

def get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: Optional[str] = None) -> Dataset:
    """Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.#

    The dataset is converted to a dictionary with the following structure:
    {
        'prompt': List[str],
        'chosen': List[str],
        'rejected': List[str],
    }

    Prompts should be structured as follows:
      \n\nHuman: <prompt>\n\nAssistant:
    Multiple turns are allowed, but the prompt should always start with \n\nHuman: and end with \n\nAssistant:.
    """
    dataset = load_dataset("Anthropic/hh-rlhf", split=split, cache_dir=cache_dir)
    if sanity_check:
        dataset = dataset.select(range(min(len(dataset), 10000)))

    def split_prompt_and_responses(sample) -> Dict[str, str]:
        prompt = extract_anthropic_prompt(sample["chosen"])
        return {
            "prompt": prompt,
            "chosen": sample["chosen"][len(prompt) :],
            "rejected": sample["rejected"][len(prompt) :],
        }

    return dataset.map(split_prompt_and_responses)

# dataset split
train_dataset = get_hh("train", sanity_check=True)
eval_dataset = get_hh("test", sanity_check=True)
eval_dataset = eval_dataset.select(range(1000))

print(train_dataset[0])

train_dataset

print(eval_dataset[0])

eval_dataset

# Create DPO trainer
dpo_trainer = DPOTrainer(
    model,
    ref_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
    beta=0.1,
    max_prompt_length=1024,
    max_length=1536,
)

# Fine-tune model with DPO
dpo_trainer.train()

# Save artifacts
dpo_trainer.model.save_pretrained("final_checkpoint")
tokenizer.save_pretrained("final_checkpoint")

# Flush memory
del dpo_trainer, model, ref_model
gc.collect()
torch.cuda.empty_cache()

# Reload model in FP16 (instead of NF4)
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    return_dict=True,
    torch_dtype=torch.float16,
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Merge base model with the adapter
model = PeftModel.from_pretrained(base_model, "final_checkpoint")
model = model.merge_and_unload()

# Save model and tokenizer
model.save_pretrained(new_model)
tokenizer.save_pretrained(new_model)

# Push them to the HF Hub
model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)
tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)

# Format prompt
message = [
    {"role": "user", "content": "What is a Large Language Model?"}
]

tokenizer = AutoTokenizer.from_pretrained(new_model)

prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)

# Create pipeline
pipeline = transformers.pipeline(
    "text-generation",
    model=new_model,
    tokenizer=tokenizer
)

# Generate text
sequences = pipeline(
    prompt,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
    max_length=200,
)
print(sequences[0]['generated_text'])

toxicity_dataset = load_dataset("Anthropic/hh-rlhf")

toxic_prompt_list = toxicity_dataset['test'].select(range(50))

def map_initial_prompts(sample):
  return {"prompt" : sample["chosen"].split("Assistant:")[0]}

toxic_prompt_list = toxic_prompt_list.map(map_initial_prompts)

print(toxic_prompt_list[0])

toxic_prompt_list[0]["prompt"]

def generate_output_from_prompt(sample, pipe):
  messages = [
      {"role": "user", "content": sample["prompt"].strip()},
  ]
  print("messages: ", messages)

  prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)

  outputs = pipe(
    prompt,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
    max_length=200,
  )
  print("outputs: ", outputs)

  return outputs[0]["generated_text"]

generate_output_from_prompt(toxic_prompt_list[0], pipeline)

base_model_generations = []

for toxic_prompt in tqdm(toxic_prompt_list):
  print("toxic_prompt: ", toxic_prompt)
  output = generate_output_from_prompt(toxic_prompt, pipeline)
  print("output: ", output)
  base_model_generations.append(output)

print(base_model_generations[0])

base_model_generations_only_completions = []

for generation in base_model_generations:
  base_model_generations_only_completions.append(generation.split("[/INST]")[-1])

print(base_model_generations_only_completions[0])

#import locale
#locale.getpreferredencoding = lambda: "UTF-8"

!pip install -qU evaluate

import evaluate

toxicity = evaluate.load("toxicity")

overall_results = toxicity.compute(predictions=base_model_generations_only_completions)

import numpy as np

# new dpo model toxicity
np.mean(overall_results['toxicity'])

orig_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    torch_dtype=torch.float16
)

orig_model.config.use_cache = True

# Format prompt
message = [
    {"role": "user", "content": "What is a Large Language Model?"}
]
tokenizer = AutoTokenizer.from_pretrained(model_name)
prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)

# Create pipeline
orig_pipeline = transformers.pipeline(
    "text-generation",
    model=orig_model,
    tokenizer=tokenizer
)

# Generate text
sequences = orig_pipeline(
    prompt,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
    max_length=200,
)
print(sequences[0]['generated_text'])

orig_model_generations = []

for toxic_prompt in tqdm(toxic_prompt_list):
  print("toxic_prompt: ", toxic_prompt)
  output = generate_output_from_prompt(toxic_prompt, orig_pipeline)
  print("output: ", output)
  orig_model_generations.append(output)

orig_model_generations_only_completions = []

for generation in orig_model_generations:
  orig_model_generations_only_completions.append(generation.split("[/INST]")[-1])

print(orig_model_generations_only_completions[0])

toxicity = evaluate.load("toxicity")

overall_results = toxicity.compute(predictions=orig_model_generations_only_completions)

np.mean(overall_results['toxicity'])

"""Evaluate the DPO trained model while loading in 4 bit"""

dpo_model = AutoModelForCausalLM.from_pretrained(
    my_dpo_model,
    torch_dtype=torch.float16,
    load_in_4bit=True
)
dpo_model.config.use_cache = True

# Format prompt
message = [
    {"role": "user", "content": "What is a Large Language Model?"}
]
tokenizer = AutoTokenizer.from_pretrained(my_dpo_model)
prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)

# Create pipeline
dpo_pipeline = transformers.pipeline(
    "text-generation",
    model=dpo_model,
    tokenizer=tokenizer
)

# Generate text
sequences = dpo_pipeline(
    prompt,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=1,
    max_length=200,
)
print(sequences[0]['generated_text'])

dpo_model_generations = []

for toxic_prompt in tqdm(toxic_prompt_list):
  print("toxic_prompt: ", toxic_prompt)
  output = generate_output_from_prompt(toxic_prompt, dpo_pipeline)
  print("output: ", output)
  dpo_model_generations.append(output)

dpo_model_generations_only_completions = []

for generation in dpo_model_generations:
  dpo_model_generations_only_completions.append(generation.split("[/INST]")[-1])

print(dpo_model_generations_only_completions[0])

toxicity = evaluate.load("toxicity")

dpo_overall_results = toxicity.compute(predictions=dpo_model_generations_only_completions)

np.mean(dpo_overall_results['toxicity'])